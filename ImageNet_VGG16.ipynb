{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### VGG16 ImageNet-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as tf, datasets \n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 지정\n",
    "ImageNet_BATCH_SIZE = 64\n",
    "\n",
    "CONV_KERNEL_SIZE = 3\n",
    "MAXP_KERNEL_SIZE = 2\n",
    "\n",
    "c_stride = 1\n",
    "c_padding = 1\n",
    "mp_stride = 2\n",
    "\n",
    "lr = 0.01\n",
    "step_size=30\n",
    "gamma= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = tf.Compose([\n",
    "    tf.Resize((224, 224)), # 이미지 사이즈 조정 \n",
    "    tf.ToTensor(), # 텐서로 변환\n",
    "    tf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 경로 지정\n",
    "Path_train_ImageNet = '../dataset/imagenet-mini/train'\n",
    "Path_test_ImageNet = '../dataset/imagenet-mini/val'\n",
    "Path_cls_ImageNet = '../dataset/imagenet-mini/imagenet_class_index.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape:  torch.Size([3, 224, 224])\n",
      "len train 34745\n",
      "len test 3923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = datasets.ImageFolder(Path_train_ImageNet)\n",
    "test_dataset = datasets.ImageFolder(Path_test_ImageNet)\n",
    "\n",
    "train_dataset.transform = transform\n",
    "test_dataset.transform = transform\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "img, _ = train_dataset[1]\n",
    "print(\"img shape: \", img.shape) # size = [3, 96, 96]\n",
    "print(\"len train\", len(train_dataset)) # 학습용 데이터 5000개\n",
    "print(\"len test\", len(test_dataset))   # 테스트 데이터 8000개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #3 224 128\n",
    "            nn.Conv2d(3, 64, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #64 112 64\n",
    "            nn.Conv2d(64, 128, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #128 56 32\n",
    "            nn.Conv2d(128, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #256 28 16\n",
    "            nn.Conv2d(256, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #512 14 8\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        #512 7 4\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(7)\n",
    "        #512 1 1\n",
    "        self.classifier = nn.Linear(512, 1000)\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(512*2*2,4096)\n",
    "        self.fc2 = nn.Linear(4096,4096)\n",
    "        self.fc3 = nn.Linear(4096,10)\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        features = self.conv(x)\n",
    "        #print(features.size())\n",
    "        x = self.avg_pool(features)\n",
    "        #print(avg_pool.size())\n",
    "        x = x.view(features.size(0), -1)\n",
    "        #print(flatten.size())\n",
    "        x = self.classifier(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [131], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m net \u001b[39m=\u001b[39m Net()\n\u001b[0;32m----> 4\u001b[0m net \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      5\u001b[0m param \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(net\u001b[39m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:612\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    610\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 612\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    361\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    361\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:381\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m param \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 381\u001b[0m         param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    382\u001b[0m     should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    383\u001b[0m     \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:610\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    609\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 610\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "param = list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
