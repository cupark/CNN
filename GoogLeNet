 1. GoogLeNet History
    2014년 ILSVRC(이미지넷 이미지 인식대회)에서 VGGNet(VGG19)를 이기고 우승한 알고리즘. 
    22개의 Layer 층으로 구성되어 있으며 논문은 'Going Deeper with Convolutions'이다. 
    
 2. GoogLeNet Architecture
    1) 1 x 1 Convolution
       - 1 x 1 Convolution은 Bottleneck의 형태를 갖기 때문에 Bottleneck Module로 이해하면된다.
       - Bottleneck에 필요한 선수지식 
         (1) Convolution Parameter = Kernel Size x Kernel Size x Input Channel x Output Channel
         (2) Kernel Size를 1을 사용하여 Input Channel에 대한 Output Channel의 결과 값만을 다룸.
         
       - Bottleneck Module을 사용하는 이유는 2가지, 유의할 점이 있다. 
           (1) Bottleneck Module 사용 이유 
               1. Kernel Size를 1로 사용하여 Parameter의 수를 감소시켜 연산량을 줄인다.
                  즉, 연산 속도를 빠르게 할 수 있다.
               2. Kernel Size를 1로 사용하여 Input Channel의 대한 Output Channel 즉 Feature Map의 변화만을 사용 가능하다.
                  즉, Channel에 대한 특징 추출이 용이하다.
           (2) Bottleneck Moduel 사용에 유의할 점
               1. Convolution Parameter의 Data가 줄어든다는 점은 데이터 손실을 의미한다. 따라서 데이터 손실(Parameter 감소)와 
                  연산 속도에 대한 보상이 서로 Tradeoff 관계이다. 그러므로 적절하게 사용해야 한다. 
       -  1 x 1 Convolution은 Feature Map의 갯수를 줄이는 목적으로 사용된다.
          ex)  5 x 5 Convolution Feature Map
               1) 특성맵 (14 x 14 x 480) 
               2) 필터 (5 x 5 x 480) x 48 
               3) Convolution Feature Map (14 x 14 x 48)
               4) 연산 횟수 = 14 x 14 x 48 x 5 x 5 x 480 = 112,9M
               
               1 x 1 Convolution Feature Map
               1) 특성맵 (14 x 14 x 480)
               2) 필터 (1 x 1 x 480) x 16
               3) Convolution Feature Map (14 x 14 x 16)
               4) 필터 (5 x 5 x 16) x 48
               5) Convolution Feature Map (14 x 14 x 48)
               6) 연산 횟수 = (14 x 14 x 16 x 1 x 1 x 480) + (14 x 14 x 48 x 5 x 5 x 16) = 5.3M
               
               ** 112M -> 5M으로 연산량을 줄일 수 있다. 연산량이 줄었다는 점에서 레이어를 더 깊게 만들 수 있다.
               
    2) Inception Module
       - GoogLeNet에서는 Inception Module이 9개 존재한다. 
       - Inception Module의 Naive 버전을 보면 1 x 1 Conv , 3 x 3 Conv, 5 x 5 Conv, 3 x 3 Maxpooling 이 존재한다. 
         여기에 1 x 1 Conv를 각각 추가하여 연산량을 줄이며 다양한 Feature Map을 활용하여 다양한 특성이 도출된다. 
         
         
    3) Global Average Pooling 
       - Fully Connected Layer와 Global Average Pooling의 차이가 있다. 
         두개 모두 1차원 벡터를 만들어준다. 1차원 벡터를 만드는 이유는 최종의 분류를 위한 Softmax 층을 연결하기 위해서다. 
         차이점은 Fully Connected Layer의 경우 가중치의 개수가 7 x 7 x 1024 Feature Map의 1024개의 가중치를 계산한 51.3M이 되지만 
         Global Average Pooling은 1024층에서 Feature Map의 가중치 평균을 사전에 구해 별도의 가중치가 필요 없다.
         
         
       - Fully Connected Layer 
         AlexNet, VGGNet 등에서 망의 후반부에 사용된다. 
         7 x 7 x 1024 -> Flatten -> 1 x 1 x 50176 -> 1 x 1 x 1024 
         
       - Global Average Pooling
         각각의 층에서의 Feature Map의 평균값을 산출하기 때문에 별도의 연산이 필요없다.
         7 x 7 x 1024에서 각각의 평균값을 제공하여 1 x 1 x 1024가 도출된다. 
         
         
    4) Auxiliary Classifier (보조 분류기)
       Vanishing gradient 문제를 회피하기 위하여 네트워크 중간에 보조 분류기를 달아 놓는다. Train 시 활성화하며 사용 시 제거한다.
       5 x 5 AveragePool(stride 3) -> 128개 1 x 1 Conv -> 1024 FC Layer -> 1000 FC Layer -> Softmax를 사용한다. 
       
         
       
       
    
               
         
