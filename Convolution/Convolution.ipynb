{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Convolution caculate Parameters**  \n",
    "---\n",
    "> #### 합성곱 연산에 따른 파라미터 계산\n",
    ">> - **Convolution Parameter**  \n",
    ">>> **1. Input Data Height : H**  \n",
    ">>> **2. Input Data Width : W**  \n",
    ">>> **3. Filter Height: FH**  \n",
    ">>> **4. Filter Width : WD**  \n",
    ">>> **5. Stride : S**  \n",
    ">>> **6. Padding : P**    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Convolution](/home/park/coding/study/DL/DeepLearning/Convolution/Convolution/outputsize.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![formula](/home/park/coding/study/DL/DeepLearning/Convolution/Convolution/formula.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> **Convolution Parameter Fomula** \n",
    ">> **InputChannel x KernelWidth x KernelHeight x OutputChannel + Bias(Filter)**   \n",
    ">>> **(Output Channel은 Filter의 값을 의미한다.)**  \n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> **BatchNormalization Parameter Configuration**   \n",
    ">> **1. Gamma: Scaling Parameter**  \n",
    ">> **2. Beta: Shift Parameter**  \n",
    ">> **3. Mean: Non-Trainable Params**  \n",
    ">> **4. Standard deviation: Non-Trainable Params**  \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Example Calculate Conv Param on AlexNet**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alexnetarchi](/home/park/coding/study/DL/DeepLearning/Convolution/Convolution/alexnet_architecture.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">### **AlexNet Architecture**\n",
    ">>- #### **Input: 227x227x3 크기의 컬러 이미지.**  \n",
    ">>> **1.Conv-1: 11x11 크기의 커널 96개, stride=4, padding=0**   \n",
    ">>> **2.MaxPool-1: stride 2, 3x3 max pooling layer**  \n",
    ">>> **3.Conv-2: 5x5 크기의 커널 256개, stride=1, padding=2**  \n",
    ">>> **4.MaxPool-2: stride 2, 3x3 max pooling layer**  \n",
    ">>> **5.Conv-3: 3x3 크기의 커널 384개, stride=1, padding=1**  \n",
    ">>> **6.Conv-4: 3x3 크기의 커널 384개, stride=1, padding=1**  \n",
    ">>> **7.Conv-5: 3x3 크기의 커널 256개, stride=1, padding=1**  \n",
    ">>> **8.Maxpool-3: stride 2, 3x3 max pooling layer**  \n",
    ">>> **9.FC-1: 4096개의 fully connected layer**  \n",
    ">>> **10.FC-2: 4096개의 fully connected layer**  \n",
    ">>> **11.FC-3: 1000개의 fully connected layer** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">### **AlexNet Architecture Code**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Class Define\n",
    "###### 논문에 W, H의 SIZE가 224로 잘못 표기 되어있어 227로 수정하여 사용한다.  \n",
    "###### 파라미터의 계산 편의를 위하여 Relu와 LRN, Dropout은 제외하여 설계하였다. (전체버전은 Github에 업로드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0, bias=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2, bias=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1,bias=True),\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1,bias=True),\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1,bias=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(in_features= 9216, out_features= 4096),\n",
    "            nn.Linear(in_features= 4096, out_features= 4096),\n",
    "            nn.Linear(in_features= 4096, out_features= 1000),            \n",
    "        )\n",
    "        \n",
    "        self.Init_Bias()\n",
    "    \n",
    "    def Init_Bias(self):\n",
    "        for layer in self.Layer:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.normal_(layer.weight, mean=0, std= 0.01)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "        nn.init.constant_(self.Layer[2].bias, 1)\n",
    "        nn.init.constant_(self.Layer[4].bias, 1)\n",
    "        nn.init.constant_(self.Layer[5].bias, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Layer(x)\n",
    "        x = x.view(-1,256 * 6 *6)\n",
    "        out = self.FC(x)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
      "         MaxPool2d-2           [-1, 96, 27, 27]               0\n",
      "            Conv2d-3          [-1, 256, 27, 27]         614,656\n",
      "         MaxPool2d-4          [-1, 256, 13, 13]               0\n",
      "            Conv2d-5          [-1, 384, 13, 13]         885,120\n",
      "            Conv2d-6          [-1, 384, 13, 13]       1,327,488\n",
      "            Conv2d-7          [-1, 256, 13, 13]         884,992\n",
      "         MaxPool2d-8            [-1, 256, 6, 6]               0\n",
      "            Linear-9                 [-1, 4096]      37,752,832\n",
      "           Linear-10                 [-1, 4096]      16,781,312\n",
      "           Linear-11                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 62,378,344\n",
      "Trainable params: 62,378,344\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 5.96\n",
      "Params size (MB): 237.95\n",
      "Estimated Total Size (MB): 244.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model1 = AlexNet()\n",
    "    summary(model=model1, input_size= (3,227,227), device= device.type)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Result\n",
    "![result1](ConvParam\\result.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">### **Calculate Number of Weights, Bias**\n",
    ">>**Weight = Size of Kernels x Size of Kernels x Number of Channels x Number of Kernel**  \n",
    ">>**Bias = Number of Channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_weights:  34848\n",
      "conv1_bias:  96\n",
      "conv1_sum:  34944\n",
      "conv2_weights:  614400\n",
      "conv2_bias:  256\n",
      "conv2_sum:  614656\n",
      "conv3_weights:  884736\n",
      "conv3_bias:  384\n",
      "conv3_sum:  885120\n",
      "conv4_weights:  1327104\n",
      "conv4_bias:  384\n",
      "conv4_sum:  1327488\n",
      "conv5_weights:  884736\n",
      "conv5_bias:  256\n",
      "conv5_sum:  884992\n"
     ]
    }
   ],
   "source": [
    "conv1_weights = 11 * 11 * 3 * 96\n",
    "conv1_bias = 96\n",
    "conv1_sum = conv1_weights + conv1_bias\n",
    "\n",
    "conv2_weights = 5 * 5 * 96 * 256\n",
    "conv2_bias = 256\n",
    "conv2_sum = conv2_weights + conv2_bias\n",
    "\n",
    "conv3_weights = 3 * 3 * 256 * 384\n",
    "conv3_bias = 384\n",
    "conv3_sum = conv3_weights + conv3_bias\n",
    "\n",
    "conv4_weights = 3 * 3 * 384 * 384\n",
    "conv4_bias = 384\n",
    "conv4_sum = conv4_weights + conv4_bias\n",
    "\n",
    "conv5_weights = 3 * 3 * 384 * 256\n",
    "conv5_bias = 256\n",
    "conv5_sum = conv5_weights + conv5_bias\n",
    "\n",
    "\n",
    "print(\"conv1_weights: \", conv1_weights)\n",
    "print(\"conv1_bias: \", conv1_bias)\n",
    "print(\"conv1_sum: \", conv1_sum)\n",
    "\n",
    "print(\"conv2_weights: \", conv2_weights)\n",
    "print(\"conv2_bias: \", conv2_bias)\n",
    "print(\"conv2_sum: \", conv2_sum)\n",
    "\n",
    "print(\"conv3_weights: \", conv3_weights)\n",
    "print(\"conv3_bias: \", conv3_bias)\n",
    "print(\"conv3_sum: \", conv3_sum)\n",
    "\n",
    "print(\"conv4_weights: \", conv4_weights)\n",
    "print(\"conv4_bias: \", conv4_bias)\n",
    "print(\"conv4_sum: \", conv4_sum)\n",
    "\n",
    "print(\"conv5_weights: \", conv5_weights)\n",
    "print(\"conv5_bias: \", conv5_bias)\n",
    "print(\"conv5_sum: \", conv5_sum)\n",
    "\n",
    "  \n",
    "        \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Result\n",
    "![result2](ConvParam\\result2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "636e982776a734e3843cf985302b4a5377d4a21d75da9d3bcc757037678bc3e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
